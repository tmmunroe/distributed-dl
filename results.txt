Files already downloaded and verified
Training Data Bytes:  614400000
[0]
Using unbounded batch size per gpu generator
BatchSize:  32 ... BatchSizePerGpu:  32
Files already downloaded and verified
Time creating dataset object:  0.9512531569998828
Epoch: 1 of 2
Epoch: 2 of 2
Epoch 2 Metrics:
Epoch 2 DataLoader Time: 5.866162172986151, Training Time: 41.55219396994653, Total Time: 47.45948182699976, Loss: 1.6489475361831354, Acc: 0.3996

BatchSize:  128 ... BatchSizePerGpu:  128
Files already downloaded and verified
Time creating dataset object:  0.9685779939991335
Epoch: 1 of 2
Epoch: 2 of 2
Epoch 2 Metrics:
Epoch 2 DataLoader Time: 7.749220448997221, Training Time: 16.109463048996986, Total Time: 23.897132226000394, Loss: 1.4547608987409242, Acc: 0.4709

BatchSize:  512 ... BatchSizePerGpu:  512
Files already downloaded and verified
Time creating dataset object:  0.9939214049991278
Epoch: 1 of 2
Epoch: 2 of 2
Epoch 2 Metrics:
Epoch 2 DataLoader Time: 14.043282651999107, Training Time: 8.069522031993984, Total Time: 22.15291468900068, Loss: 1.6980468456191247, Acc: 0.3581

BatchSize:  2048 ... BatchSizePerGpu:  2048
Files already downloaded and verified
Time creating dataset object:  0.9660029709993978
Epoch: 1 of 2
Epoch: 2 of 2
Epoch 2 Metrics:
Epoch 2 DataLoader Time: 11.475642043003973, Training Time: 8.458319806000873, Total Time: 19.972588660999463, Loss: 1.7462031153532176, Acc: 0.32684

BatchSize:  8192 ... BatchSizePerGpu:  8192
Files already downloaded and verified
Time creating dataset object:  0.9652668349990563
Epoch: 1 of 2
Epoch: 2 of 2
Epoch 2 Metrics:
Epoch 2 DataLoader Time: 13.271853626994925, Training Time: 8.56507615099872, Total Time: 21.889705247000165, Loss: 4.185181975364685, Acc: 0.20948

BatchSize:  32768 ... BatchSizePerGpu:  32768
Files already downloaded and verified
Time creating dataset object:  0.9553552010002022
Epoch: 1 of 2
Failed at batch_size_per_gpu 32768 with error: 
CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 14.56 GiB total capacity; 13.01 GiB already allocated; 106.44 MiB free; 13.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
      gpus    batch_size_per_gpu      time    communication_time    computation_time    speedup
--  ------  --------------------  --------  --------------------  ------------------  ---------
 0       1                    32  41.5522                      0            41.5522           1
 1       1                   128  16.1095                      0            16.1095           1
 2       1                   512   8.06952                     0             8.06952          1
 3       1                  2048   8.45832                     0             8.45832          1
 4       1                  8192   8.56508                     0             8.56508          1
--------------------------

--------------------------

Using batch sizes per gpu [32, 128, 512, 2048, 8192] for multi gpus...
--------------------------

--------------------------

[0, 1]
BatchSize:  64 ... BatchSizePerGpu:  32
Files already downloaded and verified
Time creating dataset object:  1.0640574240005662
Epoch: 1 of 2
Epoch: 2 of 2
Epoch 2 Metrics:
Epoch 2 DataLoader Time: 2.8366797669896187, Training Time: 49.55157125399455, Total Time: 52.43801614400036, Loss: 1.6077122121935146, Acc: 0.41506

BatchSize:  256 ... BatchSizePerGpu:  128
Files already downloaded and verified
Time creating dataset object:  0.9545317629999772
Epoch: 1 of 2
Epoch: 2 of 2
Epoch 2 Metrics:
Epoch 2 DataLoader Time: 5.368202872010443, Training Time: 20.133092960992144, Total Time: 25.54936695700053, Loss: 1.6144643071944338, Acc: 0.4268

BatchSize:  1024 ... BatchSizePerGpu:  512
Files already downloaded and verified
Time creating dataset object:  0.9576296069990349
Epoch: 1 of 2
Epoch: 2 of 2
Epoch 2 Metrics:
Epoch 2 DataLoader Time: 12.489507907994266, Training Time: 7.373060127007193, Total Time: 19.91414487700058, Loss: 1.708248007297516, Acc: 0.37066

BatchSize:  4096 ... BatchSizePerGpu:  2048
Files already downloaded and verified
Time creating dataset object:  0.9479161859999294
Epoch: 1 of 2
Epoch: 2 of 2
Epoch 2 Metrics:
Epoch 2 DataLoader Time: 16.282447711002533, Training Time: 5.449283196005126, Total Time: 21.786277277999034, Loss: 3.094898530415126, Acc: 0.18568

BatchSize:  16384 ... BatchSizePerGpu:  8192
Files already downloaded and verified
Time creating dataset object:  0.9528861149992736
Epoch: 1 of 2
Epoch: 2 of 2
Epoch 2 Metrics:
Epoch 2 DataLoader Time: 20.974076481001248, Training Time: 4.671692834999703, Total Time: 25.802267069000663, Loss: 2.5288760662078857, Acc: 0.1971

      gpus    batch_size_per_gpu      time    communication_time    computation_time    speedup
--  ------  --------------------  --------  --------------------  ------------------  ---------
 0       2                    32  49.5516              9.81376              39.7378    0.838565
 1       2                   128  20.1331              4.35754              15.7756    0.800148
 2       2                   512   7.37306             1.27053               6.10253   1.09446
 3       2                  2048   5.44928             0.225058              5.22423   1.55219
 4       2                  8192   4.67169             0.0830731             4.58862   1.8334
--------------------------

--------------------------

[0, 1, 2, 3]
BatchSize:  128 ... BatchSizePerGpu:  32
Files already downloaded and verified
Time creating dataset object:  1.0742449609988398
Epoch: 1 of 2
Epoch: 2 of 2
Epoch 2 Metrics:
Epoch 2 DataLoader Time: 2.100226538033894, Training Time: 44.50570875597077, Total Time: 46.67544243200064, Loss: 1.615594479502464, Acc: 0.40764

BatchSize:  512 ... BatchSizePerGpu:  128
Files already downloaded and verified
Time creating dataset object:  0.9624083830003656
Epoch: 1 of 2
Epoch: 2 of 2
Epoch 2 Metrics:
Epoch 2 DataLoader Time: 7.445331204997274, Training Time: 16.257794138002282, Total Time: 23.773677696999584, Loss: 1.712938211180947, Acc: 0.36112

BatchSize:  2048 ... BatchSizePerGpu:  512
Files already downloaded and verified
Time creating dataset object:  0.9396820600013598
Epoch: 1 of 2
Epoch: 2 of 2
Epoch 2 Metrics:
Epoch 2 DataLoader Time: 16.079521004996423, Training Time: 5.580504355000812, Total Time: 21.727427356998305, Loss: 1.832719046335954, Acc: 0.30828

BatchSize:  8192 ... BatchSizePerGpu:  2048
Files already downloaded and verified
Time creating dataset object:  0.9501138639989222
Epoch: 1 of 2
Epoch: 2 of 2
Epoch 2 Metrics:
Epoch 2 DataLoader Time: 15.384745606997967, Training Time: 3.3077361100004055, Total Time: 18.775931317999493, Loss: 5.880500674247742, Acc: 0.15964

BatchSize:  32768 ... BatchSizePerGpu:  8192
Files already downloaded and verified
Time creating dataset object:  0.9577957459987374
Epoch: 1 of 2
Epoch: 2 of 2
Epoch 2 Metrics:
Epoch 2 DataLoader Time: 19.473006905000148, Training Time: 2.543894881000597, Total Time: 22.129927042999043, Loss: 1.4284817377726238, Acc: 0.238

      gpus    batch_size_per_gpu      time    communication_time    computation_time    speedup
--  ------  --------------------  --------  --------------------  ------------------  ---------
 0       4                    32  44.5057               9.34475             35.161     0.933637
 1       4                   128  16.2578               3.9842              12.2736    0.990876
 2       4                   512   5.5805               0.972865             4.60764   1.44602
 3       4                  2048   3.30774              0.226637             3.0811    2.55713
 4       4                  8192   2.54389              0.033538             2.51036   3.36691
--------------------------

--------------------------

      gpus    batch_size_per_gpu      time    communication_time    total_gb    bandwidth_utilization_gb_sec
--  ------  --------------------  --------  --------------------  ----------  ------------------------------
 0       2                    32  49.5516              9.81376        0.6144                       0.0123992
 1       2                   128  20.1331              4.35754        0.6144                       0.0305169
 2       2                   512   7.37306             1.27053        0.6144                       0.0833304
 3       2                  2048   5.44928             0.225058       0.6144                       0.112749
 4       2                  8192   4.67169             0.0830731      0.6144                       0.131515
--------------------------

--------------------------

      gpus    batch_size_per_gpu      time    communication_time    total_gb    bandwidth_utilization_gb_sec
--  ------  --------------------  --------  --------------------  ----------  ------------------------------
 0       4                    32  44.5057               9.34475       0.9216                       0.0207075
 1       4                   128  16.2578               3.9842        0.9216                       0.0566867
 2       4                   512   5.5805               0.972865      0.9216                       0.165146
 3       4                  2048   3.30774              0.226637      0.9216                       0.27862
 4       4                  8192   2.54389              0.033538      0.9216                       0.362279
--------------------------

--------------------------

Files already downloaded and verified
Time creating dataset object:  0.9728297239998938
Epoch: 1 of 5
Epoch: 2 of 5
Epoch: 3 of 5
Epoch: 4 of 5
Epoch: 5 of 5
Large Batch Training Results:
      gpus    batch_size_per_gpu    total_batch_size    epoch     time    accuracy     loss
--  ------  --------------------  ------------------  -------  -------  ----------  -------
 0       4                  8192               32768        5  2.54731     0.19496  3.53601
--------------------------

--------------------------

Scaling learning rate by:  256.0
Using scaled learning rate:  25.6
Using warmup epochs:  3
Files already downloaded and verified
Time creating dataset object:  0.9537572659992293
Adjusting learning rate of group 0 to 8.5333e+00.
Epoch: 1 of 5
Adjusting learning rate of group 0 to 1.4222e+01.
Epoch: 2 of 5
Adjusting learning rate of group 0 to 1.9911e+01.
Epoch: 3 of 5
Adjusting learning rate of group 0 to 2.5600e+01.
Epoch: 4 of 5
Adjusting learning rate of group 0 to 2.5600e+01.
Epoch: 5 of 5
Adjusting learning rate of group 0 to 2.5600e+01.
Large Batch Training with Warmup and Linearly Scaled Learning Rate Results:
      gpus    batch_size_per_gpu    total_batch_size    epoch    warmup_epochs    learning_rate     time    accuracy     loss
--  ------  --------------------  ------------------  -------  ---------------  ---------------  -------  ----------  -------
 0       4                  8192               32768        5                3             25.6  2.52429     0.10996  2873.01
--------------------------

--------------------------

